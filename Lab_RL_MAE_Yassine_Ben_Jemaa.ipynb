{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eqhJt7GchZi"
      },
      "source": [
        "# Lab: MAE - Self-Supervised Training of ViT in Pytorch\n",
        "\n",
        "For any remark or suggestion on this lab, please feel free to contact me at:\n",
        "\n",
        "- loic.lefolgoc@telecom-paris.fr\n",
        "\n",
        "### Objective:\n",
        "\n",
        "We are going to pretrain a Vision Transformer (ViT) using the Masked Auto-Encoding approach, before fine-tuning it for image recognition. We will use this model on a well-known dataset: CIFAR-10 https://www.cs.toronto.edu/~kriz/cifar.html.\n",
        "\n",
        "The model will be implemented using the Pytorch environment : https://pytorch.org/.\n",
        "    \n",
        "### Your task:\n",
        "You need to add the missing parts in the code (generally marked by `...`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgyu2GBVW192"
      },
      "source": [
        "# Load packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Qj5KY79W192"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BtiOfk3m6YKp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXgy6R4hzd8X",
        "outputId": "6dbc9b52-2959-4716-99c2-1f66a7250643"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lnafXfVgbdA8"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUtggcHGSPVo",
        "outputId": "c39074ad-ee05-4126-aa4f-5598bca97505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.17.1+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch->timm)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch->timm)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->timm) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, timm\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 timm-0.9.16\n"
          ]
        }
      ],
      "source": [
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OuPmUEgpSFH5"
      },
      "outputs": [],
      "source": [
        "from timm.models.layers import trunc_normal_\n",
        "from timm.models.vision_transformer import Block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5D5BY20W1-m"
      },
      "source": [
        "# Load the dataset: CIFAR-10\n",
        "\n",
        "We are going to train a vision transformer on a well-known vision dataset : CIFAR10. CIFAR10 consists of $60,000$ $32\\times 32$ color images of $10$ different object classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5Al5b-S37T4A"
      },
      "outputs": [],
      "source": [
        "# The CIFAR10 categories\n",
        "cifar_10_list = [ 'airplane', 'automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp37mAZFTPFp"
      },
      "source": [
        "We import the CIFAR-10 data and carry out some pre-processing :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k15abDeRW1-m",
        "outputId": "04e94e13-bce8-4d4a-9a58-674c968cd21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 38071003.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "           )\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Convert input to Pytorch tensors\n",
        "input_transform=transforms.Compose( [transforms.ToTensor()])\n",
        "\n",
        "# Retrieve CIFAR training data\n",
        "cifar_trainset = datasets.CIFAR10(root='./data',train=True,download=True,transform=input_transform)\n",
        "print(cifar_trainset)\n",
        "\n",
        "# Retrieve test data\n",
        "cifar_testset = datasets.CIFAR10(root='./data',train=False,download=True,transform=input_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "E1fWrMSmR16_"
      },
      "outputs": [],
      "source": [
        "# Extract the test data and labels for later (visualization of the results)\n",
        "X_test = torch.from_numpy(cifar_testset.data/255.0).float().permute(0,3,1,2)\n",
        "Y_test = torch.tensor(cifar_testset.targets, dtype=torch.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrGI_L3OW1-3"
      },
      "source": [
        "# Build the MAE model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfbsac9FfYNE"
      },
      "source": [
        "An MAE model consists of a standard transformer encoder and of a decoder.\n",
        "\n",
        "The encoder is in charge of learning patch representations (token embeddings) that somehow capture their semantic context (the object they belong to, their situation with respect to the other tokens, etc.) The encoder does not see all patches in the image (many patches are masked).\n",
        "\n",
        "The decoder is in charge of reconstructing the full image from the reduced set of token embeddings seen by the encoder. The decoder is similar in design to the encoder, except it has a final regression head that projects each spatial token to a patch (predicting pixel values of the reconstructed image).\n",
        "\n",
        "Another difference in implementation of the MAE encoder/decoder (compared to a standard ViT encoder) is that they shuffle/mask/remove (MAE encoder), or unshuffle (MAE decoder) spatial tokens in the forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFXvjiLzkDZk"
      },
      "source": [
        "<br>Most of the pieces of code you will have to complete in the `MAE_Encoder` and `MAE_Decoder` relate to these specificities. We are going to build the model step-by-step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRo6s7Sner39"
      },
      "source": [
        "Let's complete helper functions first. Use `shuffle` and `argsort` from numpy to complete the function `random_indices` according to its description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "Fdol2YmJBQ7M"
      },
      "outputs": [],
      "source": [
        "def random_indices(size : int):\n",
        "    '''\n",
        "    Returns two outputs:\n",
        "    forward_indices: np.array (size,) An array of indices between 0 and size-1 in random order\n",
        "    backward_indices: np.array (size,) The array of indices that reorders forward_indices back to an ordered, increasing list of numbers\n",
        "      i.e. forward_indices[backward_indices] = np.arange(size)\n",
        "    '''\n",
        "    forward_indices = np.arange(size)\n",
        "    np.random.shuffle(forward_indices)\n",
        "    backward_indices = np.argsort(forward_indices)\n",
        "    return forward_indices, backward_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3-hxpS8fEpr"
      },
      "source": [
        "Use `torch.gather` to complete the helper function `take_indices` according to its description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "TXa3PAZaBRAL"
      },
      "outputs": [],
      "source": [
        "def take_indices(tokens, indices):\n",
        "    '''\n",
        "    tokens: (N, B, C) tensor, tokens[n, b, c] is the c-th entry of the n-th token in the b-th token sequence in the minibatch\n",
        "    indices: (N', B) tensor, giving the indices of the N' tokens to retrieve from the token sequence, for all token sequences in the minibatch\n",
        "\n",
        "    output: (N', B, C) tensor, corresponding to the N' tokens corresponding to the desired indices\n",
        "    '''\n",
        "\n",
        "    # Hint: use repeat (from einops) to expand indices to an (N', B, C) tensor\n",
        "    # then torch.gather to retrieve correct tokens\n",
        "    return torch.gather(tokens, 0, repeat(indices, 'n b -> n b c', c=tokens.shape[-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>Right after tokenizing an image into patches, creating the patch embeddings, and adding positional embeddings, the `MAE_Encoder` shuffles the token embeddings and discards some of them. This is done by the `PatchShuffle` module below."
      ],
      "metadata": {
        "id": "NLTcuc3lvPHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "86YL75HHBRL3"
      },
      "outputs": [],
      "source": [
        "class PatchShuffle(torch.nn.Module):\n",
        "    '''\n",
        "    Shuffles the token embeddings for a minibatch (different reordering for different minibatch samples),\n",
        "    then discards some of them.\n",
        "\n",
        "    Attributes:\n",
        "      ratio: Scalar in (0,1): the fraction of discarded patches\n",
        "    '''\n",
        "    def __init__(self, ratio) -> None:\n",
        "        super().__init__()\n",
        "        self.ratio = ratio\n",
        "\n",
        "    def forward(self, patches : torch.Tensor):\n",
        "        '''\n",
        "        Takes the token embeddings (patches) as input.\n",
        "        Generates a random reordering (forward_indices) and the mapping back to the original order (backward_indices)\n",
        "        and shuffles token embeddings accordingly. Then, discards a number of patches.\n",
        "        '''\n",
        "\n",
        "        N, B, C = patches.shape\n",
        "        N_kept = int(N * (1 - self.ratio))\n",
        "\n",
        "        # The random forward and backward indices for reordering each token sequence in the minibatch:\n",
        "        # (format: list of (forward_indices,backward_indices) tuples)\n",
        "        indices = [random_indices(N_kept)  for _ in range(B)]\n",
        "\n",
        "        # Cast them to torch tensors:\n",
        "        forward_indices = torch.as_tensor(np.stack([i[0] for i in indices], axis=-1), dtype=torch.long).to(patches.device)\n",
        "        backward_indices = torch.as_tensor(np.stack([i[1] for i in indices], axis=-1), dtype=torch.long).to(patches.device)\n",
        "\n",
        "        # Retrieve (in 'patches') the token embeddings corresponding to the N_kept first indices in forward_indices\n",
        "        patches = take_indices(patches, forward_indices)[:N_kept]\n",
        "\n",
        "        # Return\n",
        "        return patches, forward_indices, backward_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>We now turn towards implementing the `MAE_Encoder`. It contains:\n",
        "- a 'stem' `self.patchify` that turns images into token embeddings;\n",
        "- positional embeddings and the cls token;\n",
        "- transformer encoder blocks `Block` from the `timm` library (that implement the MHSA layer, feed-forward layer, and put it all together);\n",
        "- and finally, a `PatchShuffle` layer as implemented above, to shuffle and drop tokens"
      ],
      "metadata": {
        "id": "NGS_JxlkK42f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "XWcRdWSHHS73"
      },
      "outputs": [],
      "source": [
        "class MAE_Encoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_size=32,\n",
        "                 patch_size=2,\n",
        "                 emb_dim=192,\n",
        "                 mlp_ratio=4,\n",
        "                 num_layer=12,\n",
        "                 num_head=3,\n",
        "                 mask_ratio=0.75,\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.cls_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2, 1, emb_dim))\n",
        "\n",
        "        # Tokenization / patch embedding layer\n",
        "        self.patchify = torch.nn.Conv2d(3, emb_dim, patch_size, patch_size)\n",
        "\n",
        "        # Token shuffling layer\n",
        "        self.shuffle = PatchShuffle(mask_ratio)\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head, mlp_ratio=mlp_ratio) for _ in range(num_layer)])\n",
        "        self.layer_norm = torch.nn.LayerNorm(emb_dim)\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        trunc_normal_(self.cls_token, std=.02)\n",
        "        trunc_normal_(self.pos_embedding, std=.02)\n",
        "\n",
        "    def forward(self, img):\n",
        "        '''\n",
        "        Takes as input images in (B, C, H, W) format.\n",
        "        Returns the token embeddings (including the cls token) after shuffling\n",
        "        and discarding some tokens. Also returns the backward indices that allow us\n",
        "        to \"unshuffle\" tokens to their original location in the sequence\n",
        "        '''\n",
        "\n",
        "        B = img.shape[0]\n",
        "\n",
        "        # Create token embeddings from images, token embeddings should end up in (N, B, C) format\n",
        "        # so do not forget to rearrange!\n",
        "        patches = self.patchify(img)\n",
        "        patches = rearrange(patches, 'b c h w -> (h w) b c') #'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patches.shape[2], p2=patches.shape[3])\n",
        "\n",
        "        # Add positional embeddings\n",
        "        patches = patches + self.pos_embedding\n",
        "\n",
        "        # Shuffle tokens and keep only a subset\n",
        "        patches, forward_indices, backward_indices = self.shuffle(patches)\n",
        "\n",
        "        # Add class token\n",
        "        patches = torch.cat([self.cls_token.expand(-1, B, -1), patches], dim=0)\n",
        "\n",
        "        # Process through the transformer encoder\n",
        "        # (be careful, the encoder expects patches in (B, N, C) format as input!)\n",
        "        patches = rearrange(patches, 'n b c -> b n c')\n",
        "        features = self.layer_norm(self.transformer(patches))\n",
        "\n",
        "        # Reshape tokens from (B, N, C) to (N, B, C) before returning\n",
        "        features = rearrange(features, 'b n c -> n b c')\n",
        "        return features, backward_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br>Next is the `MAE_Decoder`. It contains:\n",
        "- positional embeddings, and a special learnable `mask_token` replacing the token embeddings for the tokens that were masked out in the first phase;\n",
        "- transformer decoder blocks identical to the encoder blocks *i.e.*, the same `Block` module from the `timm` library;\n",
        "- a regression head self.head, that is a `torch.nn.Linear` layer projecting $(N, B, C)$ tensors to $(N, B, C_{in})$ tensors, where $C_{in}$ is the number of entries in an RGB patch of size `patch_size`;\n",
        "- the regression head is complemented by a `self.patch2img` layer that rearranges $(N, B, C_{in})$ tensors to $(B, 3, H, W)$ tensors.\n",
        "\n",
        "In the forward pass, the `MAE_Decoder` takes the output of the `MAE_Encoder`. It generates reconstructed images and masks of the same size as reconstructed images, with 1's in the masked tokens and 0's in the retained tokens."
      ],
      "metadata": {
        "id": "QeT8bbYWbQZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "1r_vm6TRH3mm"
      },
      "outputs": [],
      "source": [
        "class MAE_Decoder(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_size=32,\n",
        "                 patch_size=2,\n",
        "                 emb_dim=192,\n",
        "                 mlp_ratio=4,\n",
        "                 num_layer=4,\n",
        "                 num_head=3,\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.mask_token = torch.nn.Parameter(torch.zeros(1, 1, emb_dim))\n",
        "        self.pos_embedding = torch.nn.Parameter(torch.zeros((image_size // patch_size) ** 2 + 1, 1, emb_dim))\n",
        "\n",
        "        # Transformer layers\n",
        "        self.transformer = torch.nn.Sequential(*[Block(emb_dim, num_head, mlp_ratio=mlp_ratio) for _ in range(num_layer)])\n",
        "\n",
        "\n",
        "        # Regression head to predict pixel values\n",
        "        self.head = torch.nn.Linear(emb_dim, 3* patch_size **2) #Enlever la dernière dimension si ca marche pas\n",
        "        self.patch2img = Rearrange('(h w) b (c p1 p2) ->b c (h p1) (w p2) ', p1=patch_size, p2=patch_size, h = image_size//patch_size, w = image_size//patch_size)\n",
        "\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def init_weight(self):\n",
        "        trunc_normal_(self.mask_token, std=.02)\n",
        "        trunc_normal_(self.pos_embedding, std=.02)\n",
        "\n",
        "    def forward(self, features, backward_indices):\n",
        "        N_, B, C = features.shape\n",
        "        N = backward_indices.shape[0]\n",
        "\n",
        "        # Adding the class token index (0) to the indices because the cls token is part of features\n",
        "        backward_indices_augmented = torch.cat([torch.zeros(1, B).to(backward_indices), backward_indices + 1], dim=0)\n",
        "\n",
        "        # Adding the mask tokens to the encoded features, then shuffling tokens\n",
        "        # back to their correct (original) positions in the token sequence\n",
        "        features = torch.cat([features, self.mask_token.expand(N + 1 - N_, B, -1)], dim=0)\n",
        "        features = take_indices(features, backward_indices_augmented)\n",
        "\n",
        "        # Adding positional embeddings\n",
        "        features = features + self.pos_embedding\n",
        "\n",
        "        # Run the tokens through the transformer decoder layers\n",
        "        features = rearrange(features, 'n b c -> b n c')\n",
        "        features = self.transformer(features)\n",
        "        features = rearrange(features, 'b n c -> n b c')\n",
        "\n",
        "        # Remove cls token from features\n",
        "        features = features[1:]\n",
        "\n",
        "        # Regression head: predict pixel values for each patch\n",
        "        patches = self.head(features)\n",
        "\n",
        "        # Create a mask of the same size as patches, filled with 0's in the tokens\n",
        "        # that we ran through the encoder and 1's in the tokens that were masked\n",
        "        mask = torch.zeros_like(patches)\n",
        "        mask[N_-1:] = 1\n",
        "        mask = take_indices(mask, backward_indices_augmented[1:] - 1) # Maybe false\n",
        "\n",
        "        # Reshape the patches and mask to images ((B, C, H, W) tensors)\n",
        "        img = self.patch2img(patches)\n",
        "        mask = self.patch2img(mask)\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ug8Hzz_crFC"
      },
      "source": [
        "Finally, the `MAE_ViT` puts it all together. It contains an `MAE_Encoder` and an `MAE_Decoder`. In the forward pass, it takes a batch of images that it passes through the encoder. The decoder then generates (from the output of the encoder) reconstructed images and the corresponding masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "QTMkx4GrRzNF"
      },
      "outputs": [],
      "source": [
        "class MAE_ViT(torch.nn.Module):\n",
        "    def __init__(self,\n",
        "                 image_size=32,\n",
        "                 patch_size=2,\n",
        "                 emb_dim=192,\n",
        "                 mlp_ratio=4,\n",
        "                 encoder_layer=12,\n",
        "                 encoder_head=3,\n",
        "                 decoder_layer=4,\n",
        "                 decoder_head=3,\n",
        "                 mask_ratio=0.75,\n",
        "                 ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = MAE_Encoder(image_size, patch_size, emb_dim, mlp_ratio, encoder_layer, encoder_head, mask_ratio)\n",
        "        self.decoder = MAE_Decoder(image_size, patch_size, emb_dim, mlp_ratio, decoder_layer, decoder_head)\n",
        "\n",
        "    def forward(self, img):\n",
        "        features, backward_indices = self.encoder(img)\n",
        "        predicted_img, mask = self.decoder(features, backward_indices)\n",
        "        return predicted_img, mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uqo3bDVJuL3F"
      },
      "source": [
        "<br>The `ViT_Classifier` below is the actual model used for classification on the downstream task, using pretrained weights from the `MAE_Encoder` (that can be further fine-tuned). It is a standard ViT, similar to the one we built in the previous lab, except that it is instantiated from an `MAE_Encoder`.\n",
        "\n",
        "Like a standard ViT, it contains a classification head `self.head`, that is a `torch.nn.Linear` layer projecting features ($(B,D)$ tensor) to unnormalized probabilities, *a.k.a.* logits ($(B, K)$ tensor where $K$ is the number of classes).\n",
        "\n",
        "Complete the missing bits in the `__init__` routine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "7SdZGjEcY4Zz"
      },
      "outputs": [],
      "source": [
        "class ViT_Classifier(torch.nn.Module):\n",
        "    '''\n",
        "    This is a ViT model similar to the one in the previous lab, except that for\n",
        "    instantiation it takes as argument an MAE encoder and copies the relevant layers.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, encoder : MAE_Encoder, num_classes=10, pool='cls') -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "        self.pool = pool\n",
        "\n",
        "        # Deep copy of the encoder that was passed as argument,\n",
        "        # so that we don't modify it if we fine-tune\n",
        "        encoder = copy.deepcopy(encoder)\n",
        "\n",
        "        # Retrieve from the encoder the relevant pre-trained layers\n",
        "        self.cls_token = encoder.cls_token\n",
        "        self.pos_embedding = encoder.pos_embedding\n",
        "        self.patchify = encoder.patchify\n",
        "        self.transformer = encoder.transformer\n",
        "        self.layer_norm = encoder.layer_norm\n",
        "\n",
        "        # Add a classification head\n",
        "        D = self.pos_embedding.shape[-1]\n",
        "        self.head = torch.nn.Linear(D, num_classes)\n",
        "\n",
        "    def forward(self, img, detach=False):\n",
        "        B = img.shape[0]\n",
        "\n",
        "        patches = self.patchify(img)\n",
        "        patches = rearrange(patches, 'b c h w -> (h w) b c')\n",
        "\n",
        "        patches = patches + self.pos_embedding\n",
        "        patches = torch.cat([self.cls_token.expand(-1, B, -1), patches], dim=0)\n",
        "        patches = rearrange(patches, 'n b c -> b n c')\n",
        "\n",
        "        features = self.layer_norm(self.transformer(patches))\n",
        "        features = rearrange(features, 'b n c -> n b c')\n",
        "        features = features.mean(dim = 0) if self.pool == 'mean' else features[0]\n",
        "\n",
        "        if detach:\n",
        "          features = features.detach()\n",
        "\n",
        "        logits = self.head(features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D1PFmAMVQGc"
      },
      "source": [
        "# Instantiate an MAE model & train it"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the GPU if it is available on your machine, otherwise the CPU. We do so by putting the model and data on the active device with `.to(device)`. GPU training is 6-10x faster here."
      ],
      "metadata": {
        "id": "05ixmtDj-SZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "wz4dE1aG-MoK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb0d8f25-dbb5-41be-bfb5-5ddb3f961df6"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PVLE8l7WL53"
      },
      "source": [
        "To keep the model trainable in limited time, we choose the following settings:\n",
        "- We tokenize the input image in $4\\times 4$ patches (3 channels)\n",
        "- We use 6 transformer encoder blocks: because MAE encoders only process a small fraction of the image patches (the non-masked ones), we can afford more encoder blocks than in the previous lab (re: computational budget); it also acts as a form of data augmentation (re: overfitting)\n",
        "- We embed the patch to $D$ dimension, where $D$ preserves the input dimension of the patch (invertible mapping)\n",
        "- We use 3 heads for multihead self-attention layers\n",
        "- We reduce the width of the hidden layers in MLP from the default $4D$ down to $2D$ to reduce the number of parameters\n",
        "\n",
        "Furthermore, we will use a masking ratio of 0.75.\n",
        "\n",
        "<br>To accelerate pre-training, I have already run pretraining for between 20 and 50 epochs for you. You can use those pre-trained weights as a starting point and just pre-train for 5 more epochs. By default, those weights are downloaded down below. You can comment that cell if you want to pre-train from scratch instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "OEs-EnXLW1-4"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-2\n",
        "weight_decay = 1e-2\n",
        "n_epochs = 20\n",
        "batch_size = 128\n",
        "\n",
        "# !! Mettre le patch size à 4 car poids importé utilise patch size de 4\n",
        "\n",
        "\n",
        "# Transformer parameters\n",
        "dim = 48        # Embedding dimension D\n",
        "enc_depth = 6        # Number of encoder blocks\n",
        "enc_heads = 3         # Number of attention heads in MHSA layers in the encoder\n",
        "dec_depth = 3  #4      # Number of decoder blocks\n",
        "dec_heads = 3          # Number of attention heads in MHSA layers in the decoder\n",
        "mlp_ratio = 2         # Dimension of the hidden layer in MLP layers / dimension of the embedding\n",
        "mask_ratio = 0.75        # Percentage of masked tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "2GXvg5HzUs_b"
      },
      "outputs": [],
      "source": [
        "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "9h3kQ1NAUtcW"
      },
      "outputs": [],
      "source": [
        "model = MAE_ViT(image_size=32, patch_size=4, emb_dim=dim, mlp_ratio=mlp_ratio, encoder_layer=enc_depth, encoder_head=enc_heads, decoder_layer=dec_depth, decoder_head=dec_heads, mask_ratio=mask_ratio).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-ylgI9KiQvi"
      },
      "source": [
        "Download and start from weights obtained after 20 epochs of pre-training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ-Jl_5Ekx3O",
        "outputId": "2094f57f-de37-4e21-ddb1-97a8f09d45ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2024-03-19 16:45:03--  https://drive.google.com/uc?export=download&id=1QYd0aOiku72uGbAiCrAmCwxIaRYxugRo\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.113, 142.251.2.101, 142.251.2.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1QYd0aOiku72uGbAiCrAmCwxIaRYxugRo&export=download [following]\n",
            "--2024-03-19 16:45:03--  https://drive.usercontent.google.com/download?id=1QYd0aOiku72uGbAiCrAmCwxIaRYxugRo&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 764711 (747K) [application/octet-stream]\n",
            "Saving to: ‘weights_20.pth’\n",
            "\n",
            "weights_20.pth      100%[===================>] 746.79K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-19 16:45:05 (4.99 MB/s) - ‘weights_20.pth’ saved [764711/764711]\n",
            "\n",
            "FINISHED --2024-03-19 16:45:05--\n",
            "Total wall clock time: 1.6s\n",
            "Downloaded: 1 files, 747K in 0.1s (4.99 MB/s)\n",
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2024-03-19 16:45:05--  https://drive.google.com/uc?export=download&id=1EuWq_vwjBOOcJC558rZFLkM0ZvZiP6aP\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.113, 142.251.2.101, 142.251.2.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1EuWq_vwjBOOcJC558rZFLkM0ZvZiP6aP&export=download [following]\n",
            "--2024-03-19 16:45:05--  https://drive.usercontent.google.com/download?id=1EuWq_vwjBOOcJC558rZFLkM0ZvZiP6aP&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 765684 (748K) [application/octet-stream]\n",
            "Saving to: ‘weights_40.pth’\n",
            "\n",
            "weights_40.pth      100%[===================>] 747.74K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-19 16:45:06 (4.90 MB/s) - ‘weights_40.pth’ saved [765684/765684]\n",
            "\n",
            "FINISHED --2024-03-19 16:45:06--\n",
            "Total wall clock time: 1.5s\n",
            "Downloaded: 1 files, 748K in 0.1s (4.90 MB/s)\n",
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2024-03-19 16:45:07--  https://drive.google.com/uc?export=download&id=1YjGuZ1OAXmqXYYEcey1mn7Cz1KvJN61P\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.2.113, 142.251.2.101, 142.251.2.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.2.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1YjGuZ1OAXmqXYYEcey1mn7Cz1KvJN61P&export=download [following]\n",
            "--2024-03-19 16:45:07--  https://drive.usercontent.google.com/download?id=1YjGuZ1OAXmqXYYEcey1mn7Cz1KvJN61P&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.251.2.132, 2607:f8b0:4023:c0d::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.251.2.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 765684 (748K) [application/octet-stream]\n",
            "Saving to: ‘weights_50.pth’\n",
            "\n",
            "weights_50.pth      100%[===================>] 747.74K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-03-19 16:45:08 (4.97 MB/s) - ‘weights_50.pth’ saved [765684/765684]\n",
            "\n",
            "FINISHED --2024-03-19 16:45:08--\n",
            "Total wall clock time: 1.4s\n",
            "Downloaded: 1 files, 748K in 0.1s (4.97 MB/s)\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1QYd0aOiku72uGbAiCrAmCwxIaRYxugRo' -O weights_20.pth\n",
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1EuWq_vwjBOOcJC558rZFLkM0ZvZiP6aP' -O weights_40.pth\n",
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1YjGuZ1OAXmqXYYEcey1mn7Cz1KvJN61P' -O weights_50.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "5FeFHPR9lTV-"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('weights_50.pth'))\n",
        "n_epochs = 5\n",
        "learning_rate = 1e-3 # reducing the learning rate if doing a warm restart"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "SAXkn--c_G6i"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QsC21adidPC"
      },
      "source": [
        "<br> Run the MAE model. The MAE loss is a Mean Squared Error loss between the reconstructed and true images, computed only on the area corresponding to the masked tokens. Divide it by `mask_ratio` to normalize it. Implement it manually using `torch.mean`, `predicted_img`, `img` and `mask`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "lUPJPurEUthg"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "XuRzDxYIa0OA",
        "outputId": "557482e9-310e-4f44-86df-4a6a1f2d238b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:   0%|          | 0/391 [00:00<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (17) must match the size of tensor b (65) at non-singleton dimension 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-48d8c85c71a8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;31m# BEGIN STUDENT CODE: forward and backward passes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mpredicted_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_img\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmask_ratio\u001b[0m  \u001b[0;31m#...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-88-e55c8e1baabf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpredicted_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredicted_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-119-e857c4248a3b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, backward_indices)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Adding positional embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# Run the tokens through the transformer decoder layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (17) must match the size of tensor b (65) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "model.train()\n",
        "\n",
        "for epoch in range(0,n_epochs):\n",
        "  train_loss=0.0\n",
        "  all_labels = []\n",
        "  all_predicted = []\n",
        "\n",
        "  with tqdm(cifar_train_loader, unit=\"batch\") as tepoch:\n",
        "    for img, label in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Putting data on device\n",
        "      img = img.to(device)\n",
        "\n",
        "      # BEGIN STUDENT CODE: forward and backward passes\n",
        "      predicted_img, mask = model.forward(img)\n",
        "\n",
        "      loss = torch.mean(torch.abs(predicted_img - img) ** 2 * mask)/mask_ratio\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # END STUDENT CODE\n",
        "\n",
        "      # Compute the loss\n",
        "      train_loss += loss.item()*len(label)\n",
        "\n",
        "  print('Epoch {}: MAE Train Loss: {:.4f}'.format(epoch, train_loss/len(cifar_train_loader.dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td5K4XTMiiio"
      },
      "source": [
        "<br>Visualize the MAE decoder's predictions on test images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1-O_liNcF-U"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  ids = np.random.choice(len(cifar_testset), 5, replace=False)\n",
        "  val_img = X_test[ids, :].to(device)\n",
        "  predicted_val_img, mask = model(val_img)\n",
        "  predicted_val_img = predicted_val_img * mask + val_img * (1 - mask)\n",
        "  img = torch.stack([val_img * (1 - mask), predicted_val_img, val_img], dim=1)\n",
        "\n",
        "plt.figure(figsize=(6, 10))\n",
        "for idx in range(0,15):\n",
        "  plt.subplot(5, 3, idx+1)\n",
        "  row = idx//3\n",
        "  col = idx - 3*row\n",
        "  plt.imshow(img[row,col,:,:,:].permute(1,2,0).detach().cpu().numpy())\n",
        "  text = \"mask\" if col==0 else (\"predicted\" if col==1 else \"image\")\n",
        "  plt.title(text)\n",
        "  plt.grid(False)\n",
        "  plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So3vIcvS7wr_"
      },
      "source": [
        "# Fine-tuning a pre-trained ViT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0c4-Bg8EOCY"
      },
      "source": [
        "Now that we have pretrained a ViT encoder using Masked Auto-Encoding, we will fine-tune it specifically for the task of object recognition (classification) on the CIFAR-10 dataset.\n",
        "\n",
        "Training the same model *from scratch* for this task, for up to 20 epochs, typically yields a test performance below 60% accuracy with clear signs of overfitting (you can also try it for yourself). Let's see if we can do better, in 5 epochs of fine-tuning, if we start from the MAE pretrained weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDK031kYafcx"
      },
      "source": [
        "<br>Firstly we will reuse two convenience functions from the previous lab:\n",
        "- to transform a class probability tensor to class labels\n",
        "- to compute the prediction accuracy from predicted and ground truth labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "WuMqpS9M2xsp"
      },
      "outputs": [],
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n",
        "  return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "wgXdu5xJ2y2E"
      },
      "outputs": [],
      "source": [
        "def prediction_accuracy(predict,labels):\n",
        "  accuracy = (predict == labels).sum()/(labels.shape[0])\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj9jj01nBMA2"
      },
      "source": [
        "<br>We define the parameters for this fine-tuning step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "AnN1Uck7817z"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-2\n",
        "n_epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "# Model parameters\n",
        "num_classes = 10\n",
        "pool = 'mean' # you can use 'cls' instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "FLX42JYv_VQD"
      },
      "outputs": [],
      "source": [
        "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ACZkTFW1-7"
      },
      "source": [
        "<br>Now, we instantiate a ViT model with the desired parameters, with the pretrained weights of the MAE pre-training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "RmbX6oq0W1-7"
      },
      "outputs": [],
      "source": [
        "# Define the ViT model (to be fine-tuned) with the correct arguments:\n",
        "cifar_model_ft = ViT_Classifier(model.encoder, num_classes=num_classes, pool=pool)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model_ft = cifar_model_ft.to(device)"
      ],
      "metadata": {
        "id": "hbVS5rObA34x"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUmQKIEe8_QU"
      },
      "source": [
        "<br>Now, we carry out training on the CIFAR10 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "LA4Tke1vZECT"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.AdamW(cifar_model_ft.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "pyEwLnLR9gv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f7dd63-b7b1-42f6-f9fc-2d64d2121bc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 391/391 [00:15<00:00, 25.04batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 1.4424\n",
            "Epoch 0: Train Accuracy: 0.4660\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 391/391 [00:14<00:00, 26.39batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.0637\n",
            "Epoch 1: Train Accuracy: 0.6143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 391/391 [00:15<00:00, 25.47batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.9457\n",
            "Epoch 2: Train Accuracy: 0.6605\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 391/391 [00:14<00:00, 26.25batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.8736\n",
            "Epoch 3: Train Accuracy: 0.6865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 391/391 [00:15<00:00, 25.50batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.8201\n",
            "Epoch 4: Train Accuracy: 0.7083\n"
          ]
        }
      ],
      "source": [
        "cifar_model_ft.train()\n",
        "\n",
        "for epoch in range(0,n_epochs):\n",
        "  train_loss=0.0\n",
        "  all_labels = []\n",
        "  all_predicted = []\n",
        "\n",
        "  with tqdm(cifar_train_loader, unit=\"batch\") as tepoch:\n",
        "    for imgs, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on device\n",
        "      imgs = imgs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # BEGIN STUDENT CODE: forward and backward passes\n",
        "      predict = cifar_model_ft.forward(imgs) # predicted logits\n",
        "      loss = criterion(predict, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      # END STUDENT CODE\n",
        "\n",
        "      # Compute the loss\n",
        "      train_loss += loss.item()\n",
        "      # Store labels and class predictions\n",
        "      all_labels.extend(labels.tolist())\n",
        "      all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(cifar_train_loader.dataset)))\n",
        "  print('Epoch {}: Train Accuracy: {:.4f}'.format(epoch, prediction_accuracy(np.array(all_predicted),np.array(all_labels))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1MCXQIcZYF5"
      },
      "source": [
        "<br>Let's compute the test accuracy, and check that it is indeed a lot better than training from scratch (<60%)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "a8VvPS8WQ92b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0e2bb2-c42d-4772-bde4-f8005df8e120"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 30.87batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.6776\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cifar_model_ft.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(cifar_test_loader, unit=\"batch\") as tepoch:\n",
        "  for imgs, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    predict=cifar_model_ft(imgs)\n",
        "    all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "test_accuracy = prediction_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLFpuasOBWcv"
      },
      "source": [
        "## Linear-Probing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDE62z0BZE3n"
      },
      "source": [
        "In the previous section, we have allowed all parameters of the ViT to be further fine-tuned given supervised data.\n",
        "\n",
        "Linear-probing is an alternative to fine-tuning. In linear-probing, most of the parameters of the transformer backbone are frozen after pre-training. The only parameters that are still trainable are those of the linear classification head. In linear-probing, the learnt model is essentially a linear classifier that takes as features a pretrained deep representation.\n",
        "\n",
        "<br>Linear probing often provides poorer results than fine-tuning (but for a smaller computational budget). This will be true in particular when doing MAE pre-training. Let's verify this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "k4oGFae7DxJI"
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-2\n",
        "n_epochs = 5\n",
        "batch_size = 128\n",
        "\n",
        "# Model parameters\n",
        "num_classes = 10\n",
        "pool = 'mean'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "PwZSOZdgTTAI"
      },
      "outputs": [],
      "source": [
        "cifar_train_loader = torch.utils.data.DataLoader(cifar_trainset, batch_size=batch_size, shuffle=True)\n",
        "cifar_test_loader = torch.utils.data.DataLoader(cifar_testset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "6NtyvkxUrB43"
      },
      "outputs": [],
      "source": [
        "#\n",
        "cifar_model_lp = ViT_Classifier(model.encoder, num_classes=num_classes, pool=pool)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cifar_model_lp = cifar_model_lp.to(device)"
      ],
      "metadata": {
        "id": "4tF4sSOJCCLl"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWWaGGaUlD8z"
      },
      "source": [
        "<br>Training proceeds similarly to the fine-tuning case, except only the parameters of the ViT `head` should be trained. You can copy-paste the code from the previous section and adapt it. In particular, you should use the `detach=True` argument in the forward call to the model, so that the transformer encoder's parameters (except the classification head) are not backpropagated to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "vdLqMtmNBgwM"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(reduction='sum')\n",
        "optimizer = torch.optim.AdamW(cifar_model_lp.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "NdVWCp7HBoDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8201f760-6528-4bc2-be8d-ef47bc98f8b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 391/391 [00:12<00:00, 30.12batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 2.0667\n",
            "Epoch 0: Train Accuracy: 0.3119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 391/391 [00:12<00:00, 31.73batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 1.8079\n",
            "Epoch 1: Train Accuracy: 0.4033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 391/391 [00:12<00:00, 30.84batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 1.6987\n",
            "Epoch 2: Train Accuracy: 0.4277\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 391/391 [00:12<00:00, 32.15batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 1.6368\n",
            "Epoch 3: Train Accuracy: 0.4399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 391/391 [00:12<00:00, 31.75batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 1.5965\n",
            "Epoch 4: Train Accuracy: 0.4490\n"
          ]
        }
      ],
      "source": [
        "# Training:\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(0, n_epochs):\n",
        "    train_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predicted = []\n",
        "\n",
        "    with tqdm(cifar_train_loader, unit=\"batch\") as tepoch:\n",
        "        for imgs, labels in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            # Put data on device\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # BEGIN STUDENT CODE: forward and backward passes\n",
        "            predict = cifar_model_lp.forward(imgs, detach=True)\n",
        "            # CE loss\n",
        "            loss = criterion(predict, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # END STUDENT CODE\n",
        "\n",
        "            # Compute the loss\n",
        "            train_loss += loss.item()\n",
        "            # Store labels and class predictions\n",
        "            all_labels.extend(labels.tolist())\n",
        "            all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "    print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss / len(cifar_train_loader.dataset)))\n",
        "    print('Epoch {}: Train Accuracy: {:.4f}'.format(epoch, prediction_accuracy(np.array(all_predicted), np.array(all_labels))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--1_eGNdllpS"
      },
      "source": [
        "<br>Compute the accuracy on the test dataset like before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "3nXoFk9tDf3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "627313a8-fe2d-4d62-f64d-b7d7a345642c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 79/79 [00:02<00:00, 36.85batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.456\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "cifar_model_lp.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(cifar_test_loader, unit=\"batch\") as tepoch:\n",
        "  for imgs, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    predict=cifar_model_lp(imgs)\n",
        "    all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "test_accuracy = prediction_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYFvJIJme2rX"
      },
      "source": [
        "We lost more than 20 points of accuracy. Still, the performance remains reasonable for a 10 class problem considering that we essentially trained a linear model.\n",
        "\n",
        "MAEs with larger decoder depth typically perform a bit better in linear-probing than MAE models with shallow decoders. Still, MAE is often not the strongest self-supervised learning approach to use in combination with linear-probing.\n",
        "\n",
        "<br>Nevertheless, it goes without saying that with a higher computational budget, a deeper/wider ViT, and proper use of data augmentation and regularization, much better results can be achieved on this CIFAR-10 dataset exactly with the type of method that we have seen today!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zx935Lw0FOcv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}